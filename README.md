# Tech Challenge - DetecÃ§Ã£o de Fraudes com Machine Learning no Databricks

## **Abstract**

Este projeto, desenvolvido como parte do Tech Challenge, visou a criaÃ§Ã£o de um modelo de Machine Learning (ML) utilizando a plataforma **Databricks**, abrangendo todo o pipeline de dados, desde a coleta atÃ© o deployment do modelo em produÃ§Ã£o. O objetivo foi construir um sistema integrado e funcional que utiliza dados reais para treinar e servir um modelo preditivo, seguindo as seguintes etapas:

1. **Coleta e Armazenamento de Dados**:  
   - Dados de transaÃ§Ãµes de cartÃ£o de crÃ©dito foram obtidos e armazenados em um **Data Lake** na estrutura do Databricks, utilizando tabelas Delta para garantir desempenho e escalabilidade.  
   - A coleta de dados foi automatizada utilizando a API do Kaggle.

2. **ExploraÃ§Ã£o e PreparaÃ§Ã£o dos Dados**:  
   - Realizada a **anÃ¡lise exploratÃ³ria de dados (EDA)** para identificar padrÃµes, lidar com dados ausentes, e entender a distribuiÃ§Ã£o das classes (fraudes vs. nÃ£o fraudes).  
   - Engenharia de features foi aplicada para remover variÃ¡veis redundantes e preparar os dados para o treinamento do modelo.

3. **Modelagem e Treinamento**:  
   - O modelo de ML foi treinado utilizando a biblioteca AutoML do Databricks, otimizando hiperparÃ¢metros e avaliando diferentes algoritmos.  
   - A mÃ©trica principal utilizada foi o **recall**, priorizando a detecÃ§Ã£o de fraudes, uma vez que estas tÃªm impacto financeiro significativo.

4. **Deployment**:  
   - O modelo treinado foi registrado e versionado no **MLflow**, integrando-o ao Unity Catalog do Databricks para gestÃ£o eficiente.  
   - Uma aplicaÃ§Ã£o de deployment foi criada para alimentar um dashboard com previsÃµes em tempo real.

5. **VisualizaÃ§Ã£o e Storytelling**:  
   - O storytelling do projeto foi apresentado em um vÃ­deo explicativo, detalhando todas as etapas realizadas, desde a coleta de dados atÃ© o deployment do modelo.  
   - O vÃ­deo inclui visualizaÃ§Ãµes criadas a partir do Databricks e do modelo treinado.

## **TÃ©cnicas e Ferramentas Utilizadas**

### 1. **Coleta e Armazenamento**
   - Uso da **API do Kaggle** para baixar dados de transaÃ§Ãµes de cartÃ£o de crÃ©dito.
   - PersistÃªncia de dados em tabelas Delta no Databricks para armazenamento otimizado.

### 2. **AnÃ¡lise e PreparaÃ§Ã£o dos Dados**
   - **EDA (Exploratory Data Analysis)**: EstatÃ­sticas descritivas, anÃ¡lise de distribuiÃ§Ãµes e correlaÃ§Ã£o entre variÃ¡veis.
   - **RemoÃ§Ã£o de Features Irrelevantes**: ExclusÃ£o de colunas redundantes que nÃ£o contribuem para a prediÃ§Ã£o de fraudes.
   - **CriaÃ§Ã£o de Features**: CriaÃ§Ã£o de variÃ¡veis derivadas como "Hour" para capturar padrÃµes temporais.

### 3. **Treinamento do Modelo**
   - **AutoML do Databricks**: Utilizado para selecionar automaticamente o melhor algoritmo e ajustar hiperparÃ¢metros com base no **recall**.
   - **Pipeline de ML**: ConstruÃ§Ã£o de pipelines que incluem transformaÃ§Ã£o de dados e avaliaÃ§Ã£o do modelo.

### 4. **Deployment**
   - **MLflow**: Registro e versionamento do modelo no Unity Catalog do Databricks.
   - **PrediÃ§Ãµes em Tempo Real**: Uso do modelo para gerar previsÃµes e alimentar um dashboard com resultados.

### 5. **VisualizaÃ§Ã£o**
   - **Matplotlib e Seaborn**: GrÃ¡ficos para visualizaÃ§Ã£o de distribuiÃ§Ãµes e correlaÃ§Ãµes.
   - **Storytelling em VÃ­deo**: ApresentaÃ§Ã£o das etapas do projeto em formato visual.

## **Plataforma Utilizada**  
O projeto foi inteiramente desenvolvido no **Databricks**, uma plataforma unificada para engenharia de dados e aprendizado de mÃ¡quina, que facilitou o armazenamento, processamento, treinamento e deployment do modelo de ML.

---

Este projeto cumpre os requisitos estabelecidos no desafio, apresentando um modelo funcional e documentado, integrado a um pipeline de dados robusto, e demonstrando sua aplicaÃ§Ã£o prÃ¡tica por meio de um dashboard e um vÃ­deo explicativo.


<details>
  <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '01_get_dados'</strong></summary>

  Tem objetivo em fazer a coleta de dados utilizando a API do Kaggle, baixando o dataset de detecÃ§Ã£o de fraudes em cartÃ£o de crÃ©dito e carregÃ¡-lo em um Dataframe Spark e persisti-lo em uma tabela delta para consumo futuro.
  
### ğŸ”§ 1. InstalaÃ§Ã£o de Pacotes
Instala bibliotecas essenciais para manipulaÃ§Ã£o de dados, visualizaÃ§Ã£o, balanceamento de classes e machine learning com PySpark:

```bash
pip install pandas matplotlib imbalanced-learn pyspark
```



### ğŸ“¦ 2. ImportaÃ§Ã£o de Bibliotecas

Importa mÃ³dulos como:

- `pandas`, `matplotlib.pyplot` â€” manipulaÃ§Ã£o e visualizaÃ§Ã£o de dados.
- `SMOTE` â€” balanceamento de classes.
- `pyspark.ml` â€” criaÃ§Ã£o de pipelines de machine learning.



### ğŸ” 3. ConfiguraÃ§Ã£o da API do Kaggle

Cria e copia o arquivo `kaggle.json` com as credenciais de acesso Ã  API do Kaggle para autenticaÃ§Ã£o segura.



### ğŸ”‘ 4. AutenticaÃ§Ã£o com a API do Kaggle

Autentica o usuÃ¡rio com a API do Kaggle usando a biblioteca `KaggleApi`.



### ğŸ“¥ 5. Download do Dataset

Baixa o dataset `mlg-ulb/creditcardfraud` diretamente do Kaggle e salva localmente no diretÃ³rio `/dbfs/FileStore/creditcard`.



### ğŸ“‚ 6. Leitura dos Dados com Spark

Carrega o arquivo CSV para um DataFrame do Spark com detecÃ§Ã£o automÃ¡tica de tipos de dados:

```python
df = spark.read.csv("file:/dbfs/FileStore/creditcard/creditcard.csv", header=True, inferSchema=True)
```



### ğŸ’¾ 7. PersistÃªncia em Tabela Delta

Salva o DataFrame como uma tabela Delta chamada `credit_card_fraud`, permitindo consultas SQL otimizadas:

```python
df.write.format("delta").mode("overwrite").saveAsTable("credit_card_fraud")
```



### ğŸ” 8. Consulta SQL

Exibe as primeiras linhas da tabela criada:

```sql
SELECT * FROM credit_card_fraud LIMIT 5
```
</details>


<details>
  <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '02_EDA'</strong></summary>

Objetivo nessa etapa Ã© explorar os dados de transaÃ§Ãµes com cartÃ£o de crÃ©dito para entender o comportamento das fraudes. Usando PySpark, analisamos as estatÃ­sticas bÃ¡sicas, verifica se hÃ¡ dados null, observa padrÃµes ao longo do tempo e compara as diferenÃ§as entre transaÃ§Ãµes legÃ­timas e fraudulentas. TambÃ©m sÃ£o gerados grÃ¡ficos para facilitar a visualizaÃ§Ã£o e entender melhor como as variÃ¡veis se relacionam com a ocorrÃªncia de fraudes. Tudo isso ajuda a preparar o terreno para a prÃ³xima etapa: treinar modelos de machine learning.

### ğŸ”§ 1. InstalaÃ§Ã£o de Pacotes
Instala bibliotecas essenciais para manipulaÃ§Ã£o de dados, visualizaÃ§Ã£o, balanceamento de classes e machine learning com PySpark:

```bash
pip install pyspark matplotlib pandas seaborn
```



### ğŸ“¦ 2. ImportaÃ§Ã£o de Bibliotecas

Importa mÃ³dulos como:

- `pandas`, `matplotlib`, `seaborn` â€” manipulaÃ§Ã£o e visualizaÃ§Ã£o de dados.
- `pyspark` â€” manipulaÃ§Ã£o de dados distribuÃ­dos



### ğŸ“‚ 3. CriaÃ§Ã£o de nova coluna, Hour

Cria uma nova coluna nomeada 'Hour' com base no tempo em segundos a partir da primeira transaÃ§Ã£o do dataset. 

```python
df = df.withColumn('Hour', floor(col('Time') / 3600).cast('int'))
```



### ğŸ” 4. EstÃ¡tisticas descritivas

GeraÃ§Ã£o de estatÃ­sticas descritivas para os atributos. Buscamos entender a distribuiÃ§Ã£o dos dados, detectar possÃ­veis outliers e verificar escalas diferentes entre as variÃ¡veis.

```python
df.describe('V1','V2','V3','V4','V5').show()
...
df.describe('V26','V27','V28', 'amount', 'time', 'hour').show()
```

### ğŸ” 5. Contagem de valores null

Verifica a existÃªncia de valores null nas colunas. Dados ausentes impactam em modelos de machine learning e precisam ser tratados adequadamente. No dataset em questÃ£o nÃ£o houve valores null.

```python
null_counts = df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in df.columns])
null_counts.show()
```

### ğŸ”¦ 6. DistribuiÃ§Ã£o das Classes

Contagem de registros por classe, onde: 0 = nÃ£o fraude; 1 = fraude. Buscamos aqui identificar desequilÃ­brio de classes. No dataset em questÃ£o observamos que ele era desbalanceado.

```python
df.groupBy("Class") \
Â Â Â  .count() \
Â Â Â  .withColumn("percentual", round((col("count") / total_contagem) * 100,2)) \
Â Â Â  .show()
```

### â™¥ï¸ 7. Matriz de CorrelaÃ§Ã£o

CÃ¡lculo da correlaÃ§Ã£o entre as variÃ¡veis do dataset e visualizaÃ§Ã£o em formato headmap. Buscamos aqui identificar variaveis redudantes ou fortemente correlacionadas com a variÃ¡vel alvo (Class), Ãºteis para o treinamento do modelo.

```python
corr = df_pandas.corr()
sns.heatmap(corr, ...)
```

### â™¥ï¸ 8. CorrelaÃ§Ã£o Individual com a VariÃ¡vel Alvo

CÃ¡lculo da correlaÃ§Ã£o de cada variÃ¡vel com a variÃ¡vel CLASS. Ajudando assim a identificar quais variÃ¡veis tÃªm maior relaÃ§Ã£o com a ocorrÃªncia de fraudes.

```python
for col_name in df.columns:
    if col_name != "Class":
        print(f"{col_name}: {df.stat.corr(col_name, 'Class')}")
```

### ğŸ”† 9. DistribuiÃ§Ã£o de densidade por variÃ¡vel e classe

Plotagem da densidade de cada variÃ¡vel, separando por classe. Permite visualizar diferenÃ§as sutis na distribuiÃ§Ã£o das variÃ¡veis entre classes, o que pode ser explorado por modelos de classificaÃ§Ã£o.

```python
for idx, feature in enumerate(var):
    sns.kdeplot(t0[feature], ...)
    sns.kdeplot(t1[feature], ...)
```

</details>


<details> <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '03_engenharia_features'</strong></summary>

Este notebook tem como objetivo realizar a engenharia de features no dataset de detecÃ§Ã£o de fraudes em cartÃµes de crÃ©dito. Ele se concentra na preparaÃ§Ã£o dos dados para a etapa de modelagem, removendo variÃ¡veis redundantes e persistindo a base final de treinamento.

ğŸ“¦ 1. ImportaÃ§Ã£o de Bibliotecas

ImportaÃ§Ã£o de mÃ³dulos para manipulaÃ§Ã£o e transformaÃ§Ã£o de dados:

```python
    pyspark.ml.feature.VectorAssembler â€” combinaÃ§Ã£o de mÃºltiplas colunas em um Ãºnico vetor.
    pyspark.ml.stat.Summarizer â€” geraÃ§Ã£o de estatÃ­sticas resumidas para os dados.
    pyspark.sql.functions â€” funÃ§Ãµes auxiliares para manipulaÃ§Ã£o de DataFrames, como col, hour e when.
```

ğŸ“‚ 2. Leitura da Tabela Delta

Carrega o dataset persistido na etapa anterior para um DataFrame Spark:

df = spark.read.table("_0903.int.cartao_fraude_treino")

ğŸ§¹ 3. RemoÃ§Ã£o de Colunas Irrelevantes

Identifica e remove colunas que possuem comportamento semelhante para as classes fraude e nÃ£o fraude, baseando-se em anÃ¡lises exploratÃ³rias anteriores. Isso reduz a dimensionalidade do dataset e minimiza o impacto de variÃ¡veis redundantes nos modelos de machine learning.

Lista de colunas removidas:

    V13, V15, V22, V25, V26, V28

lista_remover_colunas = ['V13', 'V15', 'V22', 'V25', 'V26', 'V28']
df_modelo = df.drop(*lista_remover_colunas)

ğŸ’¾ 4. PersistÃªncia da Base Final

Salva o DataFrame resultante como uma nova tabela Delta, garantindo que esteja otimizado para uso em modelos de machine learning:
```python
try:
    df.write.format("delta").mode("overwrite").saveAsTable("_0903.int.cartao_fraude_treino_final")
    print("\n7. Tabela '_0903.int.cartao_fraude_treino_final' criada com sucesso!")
except Exception as e:
    print(f"\nâš ï¸ AVISO: NÃ£o foi possÃ­vel persistir ({str(e)})")
```

Essa tabela serÃ¡ usada como entrada na etapa de treinamento do modelo, contendo apenas as features mais relevantes.
</details>


<details>
  <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '04_Treinamento_Modelo_MLflow'</strong></summary>

  O objetivo deste notebook Ã© realizar o treinamento do modelo de detecÃ§Ã£o de fraudes usando ferramentas do Databricks, como o AutoML e o MLflow. Ele automatiza o processo de seleÃ§Ã£o de modelo e rastreia experimentos, permitindo versionamento e registro de modelos no catÃ¡logo do Databricks.

### ğŸ“¦ 1. ImportaÃ§Ã£o de Bibliotecas

Importa mÃ³dulos essenciais para o treinamento e rastreamento do modelo:

- `databricks.automl` â€” biblioteca do Databricks para automatizar o treinamento e avaliaÃ§Ã£o de modelos.
- `mlflow` â€” gerenciador de experimentos para rastrear e registrar modelos.



### ğŸ“‚ 2. Carregamento da Base de Treinamento

Carrega a base de dados preparada na etapa anterior como um DataFrame Spark:

```python
df_treino = spark.read.table("_0903.int.cartao_fraude_treino_final")
```

Isso garante que a base final, contendo apenas features relevantes, seja utilizada no treinamento.



### ğŸ¤– 3. Treinamento do Modelo com AutoML

Utiliza o AutoML para realizar as seguintes etapas automaticamente:
- SeleÃ§Ã£o de algoritmos de machine learning.
- Ajuste de hiperparÃ¢metros.
- AvaliaÃ§Ã£o baseada na mÃ©trica de recall.

**ParÃ¢metros Configurados:**
- `target_col="Class"` â€” especifica a variÃ¡vel alvo.
- `timeout_minutes=20` â€” define um tempo mÃ¡ximo para a execuÃ§Ã£o do AutoML.
- `primary_metric="recall"` â€” prioriza o recall, dado o foco em detectar fraudes.
- `experiment_name` â€” nomeia o experimento para rastreamento no MLflow.

```python
summary_normalized = automl.classify(
    dataset=df_treino,
    target_col="Class",
    timeout_minutes=20,
    experiment_name=f"analise_detec_fraude_{current_datetime}",
    primary_metric="recall",
    experiment_dir="/Workspace/Groups/databricks_0903"
)
```

### ğŸ“‘ 4. Registro do Modelo no MLflow

ApÃ³s o treinamento, o modelo com melhor desempenho Ã© registrado no catÃ¡logo do Databricks para versionamento e reutilizaÃ§Ã£o. O registro Ã© feito diretamente a partir do run associado:

```python
mlflow.register_model("runs:/5d50f24b823b4b84ac56d4954e6fcff2/model", f"{catalog}.{schema}.{model_name}")
```

**Detalhes do Registro:**
- `runs:/...` â€” identifica o run do MLflow contendo o modelo treinado.
- `catalog`, `schema`, `model_name` â€” define o local onde o modelo serÃ¡ registrado no catÃ¡logo.

Este notebook facilita a automaÃ§Ã£o do ciclo de vida do modelo, desde o treinamento atÃ© o rastreamento e registro, garantindo rastreabilidade e acessibilidade no Databricks.

</details>


<details>
  <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '05_Modelagem_Deploy'</strong></summary>

  Este notebook foca no processo de deployment do modelo de detecÃ§Ã£o de fraudes no Databricks, incluindo carregamento do modelo treinado, geraÃ§Ã£o de previsÃµes e persistÃªncia dos resultados para consultas futuras.

### ğŸ“¦ 1. ConfiguraÃ§Ã£o do Ambiente

- **Upgrade de bibliotecas**: Atualiza o MLflow para a versÃ£o mais recente e reinicia o ambiente Python para aplicar mudanÃ§as.

```bash
%pip install --upgrade "mlflow-skinny[databricks]"
dbutils.library.restartPython()
```

- **ConfiguraÃ§Ã£o do Registro de Modelos**: Define o Unity Catalog como local para rastrear e gerenciar os modelos.

```python
mlflow.set_registry_uri("databricks-uc")
```

---

### ğŸ“‚ 2. Carregamento e PreparaÃ§Ã£o do Modelo

Carrega o modelo registrado no MLflow para uso em previsÃµes. A configuraÃ§Ã£o do URI do modelo permite acesso ao modelo na fase de produÃ§Ã£o.

```python
model_uri = f"models:/{model_name}@{suffix}"
modelo_fraude = mlflow.sklearn.load_model(model_uri)
```

---

### ğŸ§ª 3. Processamento e PrevisÃµes

- **SeleÃ§Ã£o de Colunas**: Define um subconjunto de colunas relevantes para a entrada do modelo.
- **GeraÃ§Ã£o de PrevisÃµes**: Utiliza o modelo carregado para prever probabilidades e classificar transaÃ§Ãµes como fraudulentas ou nÃ£o.

```python
y_deploy_proba = modelo_fraude.predict_proba(df_deploy_pandas)[:, 1]
y_deploy = (y_deploy_proba >= 0.90).astype('uint8')
```

---

### ğŸ’¾ 4. PersistÃªncia e Processamento de Resultados

- **PersistÃªncia de Resultados**: Salva os resultados das previsÃµes como uma tabela Delta para uso em anÃ¡lises e relatÃ³rios futuros.
- **Filtro de Probabilidades**: Agrupa e visualiza os dados com base nas probabilidades preditas, permitindo anÃ¡lise detalhada das distribuiÃ§Ãµes.

```python
if df_deploy.count() > 0:
    table_name = '_0903.exp.cartao_deploy_diaria'
    process_table(sdf_result, 'delta', datetime.now().strftime('%Y-%m-%d'))
```

- **VisualizaÃ§Ã£o**: Gera grÃ¡ficos para analisar a distribuiÃ§Ã£o de probabilidades.

```python
plt.bar(df_pandas['probabilidade'], df_pandas['count_high'], color='green', label='Probabilidade > 0.90')
plt.bar(df_pandas['probabilidade'], df_pandas['count_low'], color='red', label='Probabilidade <= 0.90')
```

---

Este notebook automatiza o pipeline de deployment do modelo, garantindo que as previsÃµes sejam integradas eficientemente ao sistema de produÃ§Ã£o.

</details>
