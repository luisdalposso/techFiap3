---
<details>
  <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '01_get_dados'</strong></summary>

  Tem objetivo em fazer a coleta de dados utilizando a API do Kaggle, baixando o dataset de detecÃ§Ã£o de fraudes em cartÃ£o de crÃ©dito e carregÃ¡-lo em um Dataframe Spark e persisti-lo em uma tabela delta para consumo futuro.
  
### ğŸ”§ 1. InstalaÃ§Ã£o de Pacotes
Instala bibliotecas essenciais para manipulaÃ§Ã£o de dados, visualizaÃ§Ã£o, balanceamento de classes e machine learning com PySpark:

```bash
pip install pandas matplotlib imbalanced-learn pyspark
```

---

### ğŸ“¦ 2. ImportaÃ§Ã£o de Bibliotecas

Importa mÃ³dulos como:

- `pandas`, `matplotlib.pyplot` â€” manipulaÃ§Ã£o e visualizaÃ§Ã£o de dados.
- `SMOTE` â€” balanceamento de classes.
- `pyspark.ml` â€” criaÃ§Ã£o de pipelines de machine learning.

---

### ğŸ” 3. ConfiguraÃ§Ã£o da API do Kaggle

Cria e copia o arquivo `kaggle.json` com as credenciais de acesso Ã  API do Kaggle para autenticaÃ§Ã£o segura.

---

### ğŸ”‘ 4. AutenticaÃ§Ã£o com a API do Kaggle

Autentica o usuÃ¡rio com a API do Kaggle usando a biblioteca `KaggleApi`.

---

### ğŸ“¥ 5. Download do Dataset

Baixa o dataset `mlg-ulb/creditcardfraud` diretamente do Kaggle e salva localmente no diretÃ³rio `/dbfs/FileStore/creditcard`.

---

### ğŸ“‚ 6. Leitura dos Dados com Spark

Carrega o arquivo CSV para um DataFrame do Spark com detecÃ§Ã£o automÃ¡tica de tipos de dados:

```python
df = spark.read.csv("file:/dbfs/FileStore/creditcard/creditcard.csv", header=True, inferSchema=True)
```

---

### ğŸ’¾ 7. PersistÃªncia em Tabela Delta

Salva o DataFrame como uma tabela Delta chamada `credit_card_fraud`, permitindo consultas SQL otimizadas:

```python
df.write.format("delta").mode("overwrite").saveAsTable("credit_card_fraud")
```

---

### ğŸ” 8. Consulta SQL

Exibe as primeiras linhas da tabela criada:

```sql
SELECT * FROM credit_card_fraud LIMIT 5
```

---
</details>

---
<details>
  <summary><strong>ğŸ“– Clique para expandir a explicaÃ§Ã£o do notebook '02_EDA'</strong></summary>

Objetivo nessa etapa Ã© explorar os dados de transaÃ§Ãµes com cartÃ£o de crÃ©dito para entender o comportamento das fraudes. Usando PySpark, analisamos as estatÃ­sticas bÃ¡sicas, verifica se hÃ¡ dados null, observa padrÃµes ao longo do tempo e compara as diferenÃ§as entre transaÃ§Ãµes legÃ­timas e fraudulentas. TambÃ©m sÃ£o gerados grÃ¡ficos para facilitar a visualizaÃ§Ã£o e entender melhor como as variÃ¡veis se relacionam com a ocorrÃªncia de fraudes. Tudo isso ajuda a preparar o terreno para a prÃ³xima etapa: treinar modelos de machine learning.

### ğŸ”§ 1. InstalaÃ§Ã£o de Pacotes
Instala bibliotecas essenciais para manipulaÃ§Ã£o de dados, visualizaÃ§Ã£o, balanceamento de classes e machine learning com PySpark:

```bash
pip install pyspark matplotlib pandas seaborn
```

---

### ğŸ“¦ 2. ImportaÃ§Ã£o de Bibliotecas

Importa mÃ³dulos como:

- `pandas`, `matplotlib`, `seaborn` â€” manipulaÃ§Ã£o e visualizaÃ§Ã£o de dados.
- `pyspark` â€” manipulaÃ§Ã£o de dados distribuÃ­dos

---

### ğŸ“‚ 3. CriaÃ§Ã£o de nova coluna, Hour

Cria uma nova coluna nomeada 'Hour' com base no tempo em segundos a partir da primeira transaÃ§Ã£o do dataset. 

```python
df = df.withColumn('Hour', floor(col('Time') / 3600).cast('int'))
```

---

### ğŸ” 4. EstÃ¡tisticas descritivas

GeraÃ§Ã£o de estatÃ­sticas descritivas para os atributos. Buscamos entender a distribuiÃ§Ã£o dos dados, detectar possÃ­veis outliers e verificar escalas diferentes entre as variÃ¡veis.

```python
df.describe('V1','V2','V3','V4','V5').show()
...
df.describe('V26','V27','V28', 'amount', 'time', 'hour').show()
```

### ğŸ” 5. Contagem de valores null

Verifica a existÃªncia de valores null nas colunas. Dados ausentes impactam em modelos de machine learning e precisam ser tratados adequadamente. No dataset em questÃ£o nÃ£o houve valores null.

```python
null_counts = df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in df.columns])
null_counts.show()
```

### ğŸ”¦ 6. DistribuiÃ§Ã£o das Classes

Contagem de registros por classe, onde: 0 = nÃ£o fraude; 1 = fraude. Buscamos aqui identificar desequilÃ­brio de classes. No dataset em questÃ£o observamos que ele era desbalanceado.

```python
df.groupBy("Class") \
Â Â Â  .count() \
Â Â Â  .withColumn("percentual", round((col("count") / total_contagem) * 100,2)) \
Â Â Â  .show()
```

### â™¥ï¸ 7. Matriz de CorrelaÃ§Ã£o

CÃ¡lculo da correlaÃ§Ã£o entre as variÃ¡veis do dataset e visualizaÃ§Ã£o em formato headmap. Buscamos aqui identificar variaveis redudantes ou fortemente correlacionadas com a variÃ¡vel alvo (Class), Ãºteis para o treinamento do modelo.

```python
corr = df_pandas.corr()
sns.heatmap(corr, ...)
```

### â™¥ï¸ 8. CorrelaÃ§Ã£o Individual com a VariÃ¡vel Alvo

CÃ¡lculo da correlaÃ§Ã£o de cada variÃ¡vel com a variÃ¡vel CLASS. Ajudando assim a identificar quais variÃ¡veis tÃªm maior relaÃ§Ã£o com a ocorrÃªncia de fraudes.

```python
for col_name in df.columns:
    if col_name != "Class":
        print(f"{col_name}: {df.stat.corr(col_name, 'Class')}")
```

### ğŸ”† 9. DistribuiÃ§Ã£o de densidade por variÃ¡vel e classe

Plotagem da densidade de cada variÃ¡vel, separando por classe. Permite visualizar diferenÃ§as sutis na distribuiÃ§Ã£o das variÃ¡veis entre classes, o que pode ser explorado por modelos de classificaÃ§Ã£o.

```python
for idx, feature in enumerate(var):
    sns.kdeplot(t0[feature], ...)
    sns.kdeplot(t1[feature], ...)
```

</details>
